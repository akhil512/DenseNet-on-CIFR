{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK3alCdFflQX"
   },
   "source": [
    "### CNN on CIFR Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19164,
     "status": "ok",
     "timestamp": 1650995394880,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "6ZUZNSIHhRa1",
    "outputId": "46d5a4ac-3fc0-4112-af03-9a851df82e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use DropOut layers.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4300,
     "status": "ok",
     "timestamp": 1650995399177,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "Rij9BhMGABGC",
    "outputId": "f32c9855-7ffc-4c24-f76e-96746399f0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 22.1 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 17.9 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 9.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40 kB 8.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51 kB 4.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61 kB 5.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 5.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81 kB 5.5 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92 kB 6.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 276 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 358 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 440 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 471 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 522 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 552 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 604 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 634 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 686 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 716 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 747 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 768 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 798 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 829 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 849 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 880 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 931 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 942 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 962 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 993 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1 MB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2999,
     "status": "ok",
     "timestamp": 1650995402160,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "TLVcyNYKflQi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650995402161,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "ga9MxyF7_d3n"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650995402161,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "tuU49-8GpRbO"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "# epochs = 300\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5581,
     "status": "ok",
     "timestamp": 1650995407736,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "YOkyiQSFfaAG",
    "outputId": "c815be5b-b8ac-48f6-80c9-b8cd40162cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "170508288/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650995407737,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "6UBi4uq2nLyY"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = layers.Flatten()(AvgPooling)\n",
    "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6663,
     "status": "ok",
     "timestamp": 1650995414395,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "f4SMissbnsF_"
   },
   "outputs": [],
   "source": [
    "num_filter = 36\n",
    "dropout_rate = 0\n",
    "l = 12\n",
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2728,
     "status": "ok",
     "timestamp": 1650995417118,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "B50dRkYfpcbE",
    "outputId": "48ec1b62-edd7-4f33-d146-d13cb582c125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 36)   972         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 36)  144         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 36)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 18)   5832        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 54)   0           ['conv2d[0][0]',                 \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 54)  216         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 54)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 18)   8748        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate[0][0]',            \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 18)   11664       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 90)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 90)  360         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 90)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 18)   14580       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_2[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 18)   17496       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 126)  0           ['concatenate_3[0][0]',          \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 126)  504        ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 126)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 18)   20412       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_4[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 144)  576        ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 144)  0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 18)   23328       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 162)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 162)  648        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 162)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 18)   26244       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 180)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 180)  720        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 180)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 18)   29160       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 198)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 198)  792        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 198)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 18)   32076       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 216)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 216)  864        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 216)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 18)   34992       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 234)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 234)  936        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 234)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 18)   37908       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 252)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 252)  1008       ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 252)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 18)   4536        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 18)  0           ['conv2d_13[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 18)  72          ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 18)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 18)   2916        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 36)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 36)  144         ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 18)   5832        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 54)   0           ['concatenate_12[0][0]',         \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 54)  216         ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 54)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 18)   8748        ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_13[0][0]',         \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 72)  288         ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 18)   11664       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 90)   0           ['concatenate_14[0][0]',         \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 90)  360         ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 90)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 18)   14580       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 108)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 108)  432        ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 108)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 18)   17496       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 126)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 126)  504        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 126)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 18)   20412       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 144)  576        ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 18)   23328       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 162)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 162)  648        ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 162)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 18)   26244       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 180)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 180)  720        ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 18)   29160       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 198)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 198)  792        ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 198)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 18)   32076       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 216)  864        ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 18)   34992       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 234)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 234)  936        ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 234)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 18)   4212        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 18)    0           ['conv2d_26[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 18)    72          ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 18)     0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 18)     2916        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 36)     0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 36)    144         ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 18)     5832        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 54)     0           ['concatenate_24[0][0]',         \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 54)    216         ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 54)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 18)     8748        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_25[0][0]',         \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 72)    288         ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 18)     11664       ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 90)     0           ['concatenate_26[0][0]',         \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 90)    360         ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 90)     0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 18)     14580       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 108)    0           ['concatenate_27[0][0]',         \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 108)   432         ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 108)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 18)     17496       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 126)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 126)   504         ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 126)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 18)     20412       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 144)   576         ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 18)     23328       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 162)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 162)   648         ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 162)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 18)     26244       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 180)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 180)   720         ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 180)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 18)     29160       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 198)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 198)   792         ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 198)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 18)     32076       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 216)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 216)   864         ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 216)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 18)     34992       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 234)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'conv2d_38[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 234)   936         ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 234)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 18)     4212        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 18)    0           ['conv2d_39[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 4, 4, 18)    72          ['average_pooling2d_2[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 4, 4, 18)     0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 4, 4, 18)     2916        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_36 (Concatenate)   (None, 4, 4, 36)     0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'conv2d_40[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 4, 4, 36)    144         ['concatenate_36[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 4, 4, 18)     5832        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_37 (Concatenate)   (None, 4, 4, 54)     0           ['concatenate_36[0][0]',         \n",
      "                                                                  'conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 4, 4, 54)    216         ['concatenate_37[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 4, 4, 54)     0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 4, 4, 18)     8748        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_38 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_37[0][0]',         \n",
      "                                                                  'conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 4, 4, 72)    288         ['concatenate_38[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 4, 4, 18)     11664       ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_39 (Concatenate)   (None, 4, 4, 90)     0           ['concatenate_38[0][0]',         \n",
      "                                                                  'conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 4, 4, 90)    360         ['concatenate_39[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 4, 4, 90)     0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 4, 4, 18)     14580       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_40 (Concatenate)   (None, 4, 4, 108)    0           ['concatenate_39[0][0]',         \n",
      "                                                                  'conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 4, 4, 108)   432         ['concatenate_40[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 4, 4, 108)    0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 4, 4, 18)     17496       ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_41 (Concatenate)   (None, 4, 4, 126)    0           ['concatenate_40[0][0]',         \n",
      "                                                                  'conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 4, 4, 126)   504         ['concatenate_41[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 4, 4, 126)    0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 4, 4, 18)     20412       ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_42 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_41[0][0]',         \n",
      "                                                                  'conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 4, 4, 144)   576         ['concatenate_42[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 4, 4, 18)     23328       ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_43 (Concatenate)   (None, 4, 4, 162)    0           ['concatenate_42[0][0]',         \n",
      "                                                                  'conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 4, 4, 162)   648         ['concatenate_43[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 4, 4, 162)    0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 4, 4, 18)     26244       ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_44 (Concatenate)   (None, 4, 4, 180)    0           ['concatenate_43[0][0]',         \n",
      "                                                                  'conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 4, 4, 180)   720         ['concatenate_44[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 4, 4, 180)    0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 4, 4, 18)     29160       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 4, 4, 198)    0           ['concatenate_44[0][0]',         \n",
      "                                                                  'conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 4, 4, 198)   792         ['concatenate_45[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 4, 4, 198)    0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 4, 4, 18)     32076       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 4, 4, 216)    0           ['concatenate_45[0][0]',         \n",
      "                                                                  'conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 4, 4, 216)   864         ['concatenate_46[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 4, 4, 216)    0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 4, 4, 18)     34992       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_47 (Concatenate)   (None, 4, 4, 234)    0           ['concatenate_46[0][0]',         \n",
      "                                                                  'conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 4, 4, 234)   936         ['concatenate_47[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 4, 4, 234)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 2, 2, 234)   0           ['activation_51[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 936)          0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           9370        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 995,230\n",
      "Trainable params: 981,658\n",
      "Non-trainable params: 13,572\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1650995417118,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "t_R8xQKuqhO0",
    "outputId": "b343acae-1db8-4acd-a248-381adbeee8cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=20,width_shift_range=0.2,height_shift_range=0.2,featurewise_std_normalization=True,horizontal_flip=True,fill_mode='nearest',zoom_range=0.2,shear_range=0.2)\n",
    "gen_flow = datagen.flow(X_train,y_train,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1650999965238,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "OSgkEyOE5bqG"
   },
   "outputs": [],
   "source": [
    "def decay_learning(epochs,l_rate):\n",
    "  if epochs>1:\n",
    "    if (epochs+1)%3==0:\n",
    "      l_rate = l_rate-(l_rate*0.1)\n",
    "      print('every 5th epoch learning rate is decreased by 5%')\n",
    "  return l_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650999965239,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "Pj-weyj85YxZ"
   },
   "outputs": [],
   "source": [
    "filepath = \"/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=1)\n",
    "# Reducelr_onplateau = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,verbose=1)\n",
    "term_nan = TerminateOnNaN()\n",
    "log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1,write_graph=True)\n",
    "lr_scheduled = LearningRateScheduler(decay_learning,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650994911754,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "FDBSuJSrBNMS"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(0.01, momentum = 0.7),loss=\"categorical_crossentropy\",metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6668203,
     "status": "ok",
     "timestamp": 1650965653122,
     "user": {
      "displayName": "ml applied",
      "userId": "11299516145575697892"
     },
     "user_tz": -330
    },
    "id": "Lln2um1t-FuQ",
    "outputId": "a600bd13-1002-466c-afe6-7564e349fc15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6892 - accuracy: 0.3751\n",
      "Epoch 1: val_loss improved from inf to 1.51292, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-01.hdf5\n",
      "391/391 [==============================] - 247s 581ms/step - loss: 1.6892 - accuracy: 0.3751 - val_loss: 1.5129 - val_accuracy: 0.4460\n",
      "Epoch 2/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4138 - accuracy: 0.4868\n",
      "Epoch 2: val_loss did not improve from 1.51292\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 1.4138 - accuracy: 0.4868 - val_loss: 1.6239 - val_accuracy: 0.4481\n",
      "Epoch 3/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2644 - accuracy: 0.5424\n",
      "Epoch 3: val_loss improved from 1.51292 to 1.50090, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-03.hdf5\n",
      "391/391 [==============================] - 220s 562ms/step - loss: 1.2644 - accuracy: 0.5424 - val_loss: 1.5009 - val_accuracy: 0.5016\n",
      "Epoch 4/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.1649 - accuracy: 0.5805\n",
      "Epoch 4: val_loss did not improve from 1.50090\n",
      "391/391 [==============================] - 219s 559ms/step - loss: 1.1649 - accuracy: 0.5805 - val_loss: 1.9283 - val_accuracy: 0.4617\n",
      "Epoch 5/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.6151\n",
      "Epoch 5: val_loss improved from 1.50090 to 1.08448, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-05.hdf5\n",
      "391/391 [==============================] - 219s 561ms/step - loss: 1.0816 - accuracy: 0.6151 - val_loss: 1.0845 - val_accuracy: 0.6258\n",
      "Epoch 6/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.0123 - accuracy: 0.6396\n",
      "Epoch 6: val_loss did not improve from 1.08448\n",
      "391/391 [==============================] - 218s 559ms/step - loss: 1.0123 - accuracy: 0.6396 - val_loss: 1.2950 - val_accuracy: 0.5621\n",
      "Epoch 7/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9634 - accuracy: 0.6578\n",
      "Epoch 7: val_loss improved from 1.08448 to 1.03150, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-07.hdf5\n",
      "391/391 [==============================] - 219s 561ms/step - loss: 0.9634 - accuracy: 0.6578 - val_loss: 1.0315 - val_accuracy: 0.6482\n",
      "Epoch 8/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9063 - accuracy: 0.6797\n",
      "Epoch 8: val_loss improved from 1.03150 to 0.83461, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-08.hdf5\n",
      "391/391 [==============================] - 225s 577ms/step - loss: 0.9063 - accuracy: 0.6797 - val_loss: 0.8346 - val_accuracy: 0.7044\n",
      "Epoch 9/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8669 - accuracy: 0.6916\n",
      "Epoch 9: val_loss did not improve from 0.83461\n",
      "391/391 [==============================] - 219s 559ms/step - loss: 0.8669 - accuracy: 0.6916 - val_loss: 1.0440 - val_accuracy: 0.6496\n",
      "Epoch 10/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8331 - accuracy: 0.7069\n",
      "Epoch 10: val_loss did not improve from 0.83461\n",
      "391/391 [==============================] - 225s 574ms/step - loss: 0.8331 - accuracy: 0.7069 - val_loss: 0.9537 - val_accuracy: 0.6754\n",
      "Epoch 11/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8000 - accuracy: 0.7188\n",
      "Epoch 11: val_loss did not improve from 0.83461\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.8000 - accuracy: 0.7188 - val_loss: 0.8570 - val_accuracy: 0.7020\n",
      "Epoch 12/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7687 - accuracy: 0.7292\n",
      "Epoch 12: val_loss improved from 0.83461 to 0.78137, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-12.hdf5\n",
      "391/391 [==============================] - 226s 577ms/step - loss: 0.7687 - accuracy: 0.7292 - val_loss: 0.7814 - val_accuracy: 0.7376\n",
      "Epoch 13/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.7426\n",
      "Epoch 13: val_loss did not improve from 0.78137\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.7374 - accuracy: 0.7426 - val_loss: 0.8570 - val_accuracy: 0.7219\n",
      "Epoch 14/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7122 - accuracy: 0.7497\n",
      "Epoch 14: val_loss did not improve from 0.78137\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.7122 - accuracy: 0.7497 - val_loss: 0.8990 - val_accuracy: 0.7121\n",
      "Epoch 15/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.7583\n",
      "Epoch 15: val_loss did not improve from 0.78137\n",
      "391/391 [==============================] - 219s 561ms/step - loss: 0.6933 - accuracy: 0.7583 - val_loss: 0.8051 - val_accuracy: 0.7268\n",
      "Epoch 16/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6758 - accuracy: 0.7640\n",
      "Epoch 16: val_loss improved from 0.78137 to 0.69770, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-16.hdf5\n",
      "391/391 [==============================] - 220s 562ms/step - loss: 0.6758 - accuracy: 0.7640 - val_loss: 0.6977 - val_accuracy: 0.7690\n",
      "Epoch 17/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.7713\n",
      "Epoch 17: val_loss did not improve from 0.69770\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.6561 - accuracy: 0.7713 - val_loss: 0.9304 - val_accuracy: 0.7039\n",
      "Epoch 18/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.7770\n",
      "Epoch 18: val_loss did not improve from 0.69770\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.6396 - accuracy: 0.7770 - val_loss: 0.8954 - val_accuracy: 0.7130\n",
      "Epoch 19/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6274 - accuracy: 0.7826\n",
      "Epoch 19: val_loss did not improve from 0.69770\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.6274 - accuracy: 0.7826 - val_loss: 0.8255 - val_accuracy: 0.7282\n",
      "Epoch 20/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6082 - accuracy: 0.7880\n",
      "Epoch 20: val_loss improved from 0.69770 to 0.61071, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-20.hdf5\n",
      "391/391 [==============================] - 220s 562ms/step - loss: 0.6082 - accuracy: 0.7880 - val_loss: 0.6107 - val_accuracy: 0.7820\n",
      "Epoch 21/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.7938\n",
      "Epoch 21: val_loss did not improve from 0.61071\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5956 - accuracy: 0.7938 - val_loss: 0.8145 - val_accuracy: 0.7381\n",
      "Epoch 22/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.7990\n",
      "Epoch 22: val_loss did not improve from 0.61071\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5778 - accuracy: 0.7990 - val_loss: 0.9448 - val_accuracy: 0.6955\n",
      "Epoch 23/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.8015\n",
      "Epoch 23: val_loss did not improve from 0.61071\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5667 - accuracy: 0.8015 - val_loss: 0.8857 - val_accuracy: 0.7320\n",
      "Epoch 24/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.8034\n",
      "Epoch 24: val_loss did not improve from 0.61071\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5601 - accuracy: 0.8034 - val_loss: 0.7062 - val_accuracy: 0.7651\n",
      "Epoch 25/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.8080\n",
      "Epoch 25: val_loss did not improve from 0.61071\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5476 - accuracy: 0.8080 - val_loss: 0.8270 - val_accuracy: 0.7364\n",
      "Epoch 26/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.8146\n",
      "Epoch 26: val_loss improved from 0.61071 to 0.57993, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-26.hdf5\n",
      "391/391 [==============================] - 220s 562ms/step - loss: 0.5344 - accuracy: 0.8146 - val_loss: 0.5799 - val_accuracy: 0.8019\n",
      "Epoch 27/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.8180\n",
      "Epoch 27: val_loss did not improve from 0.57993\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.5259 - accuracy: 0.8180 - val_loss: 0.8324 - val_accuracy: 0.7416\n",
      "Epoch 28/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5184 - accuracy: 0.8194\n",
      "Epoch 28: val_loss did not improve from 0.57993\n",
      "391/391 [==============================] - 225s 575ms/step - loss: 0.5184 - accuracy: 0.8194 - val_loss: 0.8587 - val_accuracy: 0.7433\n",
      "Epoch 29/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.8247\n",
      "Epoch 29: val_loss did not improve from 0.57993\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.5079 - accuracy: 0.8247 - val_loss: 0.7545 - val_accuracy: 0.7570\n",
      "Epoch 30/30\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5005 - accuracy: 0.8261\n",
      "Epoch 30: val_loss did not improve from 0.57993\n",
      "391/391 [==============================] - 219s 560ms/step - loss: 0.5005 - accuracy: 0.8261 - val_loss: 0.6436 - val_accuracy: 0.7881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a30032450>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(gen_flow, epochs=30, validation_data=(X_test,y_test),callbacks=[checkpoint,term_nan,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zztd1Dgc-MUe"
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"/content/drive/MyDrive/24-Assignment/model-.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6698274,
     "status": "ok",
     "timestamp": 1650972680008,
     "user": {
      "displayName": "ml applied",
      "userId": "11299516145575697892"
     },
     "user_tz": -330
    },
    "id": "wXU5OPUnHkdA",
    "outputId": "c6a94a76-732d-478b-a042-e22454396ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.8167\n",
      "Epoch 1: val_loss did not improve from 0.57993\n",
      "390/390 [==============================] - 225s 566ms/step - loss: 0.5257 - accuracy: 0.8167 - val_loss: 0.8424 - val_accuracy: 0.7374\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.8191\n",
      "Epoch 2: val_loss did not improve from 0.57993\n",
      "390/390 [==============================] - 225s 577ms/step - loss: 0.5210 - accuracy: 0.8191 - val_loss: 0.9106 - val_accuracy: 0.7276\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.5083 - accuracy: 0.8235\n",
      "Epoch 3: val_loss did not improve from 0.57993\n",
      "390/390 [==============================] - 220s 562ms/step - loss: 0.5083 - accuracy: 0.8235 - val_loss: 0.6070 - val_accuracy: 0.8020\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.8255\n",
      "Epoch 4: val_loss did not improve from 0.57993\n",
      "390/390 [==============================] - 219s 562ms/step - loss: 0.5028 - accuracy: 0.8255 - val_loss: 0.6193 - val_accuracy: 0.7956\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8286\n",
      "Epoch 5: val_loss improved from 0.57993 to 0.50872, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-05.hdf5\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.4932 - accuracy: 0.8286 - val_loss: 0.5087 - val_accuracy: 0.8316\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4868 - accuracy: 0.8292\n",
      "Epoch 6: val_loss did not improve from 0.50872\n",
      "390/390 [==============================] - 220s 562ms/step - loss: 0.4868 - accuracy: 0.8292 - val_loss: 0.6437 - val_accuracy: 0.7936\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4757 - accuracy: 0.8340\n",
      "Epoch 7: val_loss did not improve from 0.50872\n",
      "390/390 [==============================] - 220s 562ms/step - loss: 0.4757 - accuracy: 0.8340 - val_loss: 0.5091 - val_accuracy: 0.8256\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.8354\n",
      "Epoch 8: val_loss did not improve from 0.50872\n",
      "390/390 [==============================] - 220s 562ms/step - loss: 0.4731 - accuracy: 0.8354 - val_loss: 0.6787 - val_accuracy: 0.7789\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8383\n",
      "Epoch 9: val_loss improved from 0.50872 to 0.50859, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-09.hdf5\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.4636 - accuracy: 0.8383 - val_loss: 0.5086 - val_accuracy: 0.8257\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.8408\n",
      "Epoch 10: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 220s 563ms/step - loss: 0.4582 - accuracy: 0.8408 - val_loss: 0.6337 - val_accuracy: 0.7937\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8454\n",
      "Epoch 11: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.4474 - accuracy: 0.8454 - val_loss: 0.9297 - val_accuracy: 0.7256\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8449\n",
      "Epoch 12: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.4469 - accuracy: 0.8449 - val_loss: 0.5798 - val_accuracy: 0.8140\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.8467\n",
      "Epoch 13: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.4414 - accuracy: 0.8467 - val_loss: 0.7392 - val_accuracy: 0.7699\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.8512\n",
      "Epoch 14: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.4318 - accuracy: 0.8512 - val_loss: 0.7791 - val_accuracy: 0.7594\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.8514\n",
      "Epoch 15: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.4259 - accuracy: 0.8514 - val_loss: 0.9253 - val_accuracy: 0.7355\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8542\n",
      "Epoch 16: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 225s 576ms/step - loss: 0.4199 - accuracy: 0.8542 - val_loss: 0.5429 - val_accuracy: 0.8282\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4159 - accuracy: 0.8552\n",
      "Epoch 17: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.4159 - accuracy: 0.8552 - val_loss: 0.5796 - val_accuracy: 0.8221\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8533\n",
      "Epoch 18: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.4164 - accuracy: 0.8533 - val_loss: 0.5590 - val_accuracy: 0.8170\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4110 - accuracy: 0.8579\n",
      "Epoch 19: val_loss did not improve from 0.50859\n",
      "390/390 [==============================] - 225s 575ms/step - loss: 0.4110 - accuracy: 0.8579 - val_loss: 0.5322 - val_accuracy: 0.8256\n",
      "Epoch 20/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.8599\n",
      "Epoch 20: val_loss improved from 0.50859 to 0.48541, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-20.hdf5\n",
      "390/390 [==============================] - 220s 563ms/step - loss: 0.4012 - accuracy: 0.8599 - val_loss: 0.4854 - val_accuracy: 0.8403\n",
      "Epoch 21/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3986 - accuracy: 0.8594\n",
      "Epoch 21: val_loss did not improve from 0.48541\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.3986 - accuracy: 0.8594 - val_loss: 0.6482 - val_accuracy: 0.7976\n",
      "Epoch 22/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8613\n",
      "Epoch 22: val_loss did not improve from 0.48541\n",
      "390/390 [==============================] - 225s 576ms/step - loss: 0.3949 - accuracy: 0.8613 - val_loss: 0.5788 - val_accuracy: 0.8109\n",
      "Epoch 23/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8652\n",
      "Epoch 23: val_loss did not improve from 0.48541\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.3898 - accuracy: 0.8652 - val_loss: 0.5127 - val_accuracy: 0.8360\n",
      "Epoch 24/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3847 - accuracy: 0.8667\n",
      "Epoch 24: val_loss did not improve from 0.48541\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.3847 - accuracy: 0.8667 - val_loss: 0.5053 - val_accuracy: 0.8355\n",
      "Epoch 25/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.8677\n",
      "Epoch 25: val_loss improved from 0.48541 to 0.45637, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-25.hdf5\n",
      "390/390 [==============================] - 225s 577ms/step - loss: 0.3816 - accuracy: 0.8677 - val_loss: 0.4564 - val_accuracy: 0.8560\n",
      "Epoch 26/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3779 - accuracy: 0.8675\n",
      "Epoch 26: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.3779 - accuracy: 0.8675 - val_loss: 0.4847 - val_accuracy: 0.8403\n",
      "Epoch 27/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8714\n",
      "Epoch 27: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 560ms/step - loss: 0.3698 - accuracy: 0.8714 - val_loss: 0.4973 - val_accuracy: 0.8442\n",
      "Epoch 28/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8702\n",
      "Epoch 28: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.3726 - accuracy: 0.8702 - val_loss: 0.5570 - val_accuracy: 0.8206\n",
      "Epoch 29/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8717\n",
      "Epoch 29: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.3671 - accuracy: 0.8717 - val_loss: 0.4660 - val_accuracy: 0.8501\n",
      "Epoch 30/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3660 - accuracy: 0.8736\n",
      "Epoch 30: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.3660 - accuracy: 0.8736 - val_loss: 0.5530 - val_accuracy: 0.8265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29c0690650>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restoring the last model\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-26.hdf5')\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,earlystop])\n",
    "# model.save_weights(os.path.join(path, '72epochs.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6659600,
     "status": "ok",
     "timestamp": 1650979340025,
     "user": {
      "displayName": "ml applied",
      "userId": "11299516145575697892"
     },
     "user_tz": -330
    },
    "id": "INgz2UjGbrGo",
    "outputId": "afcbf653-ca77-4cc7-8052-a8bc6f2e9837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8683\n",
      "Epoch 1: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 224s 564ms/step - loss: 0.3749 - accuracy: 0.8683 - val_loss: 0.5707 - val_accuracy: 0.8190\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3731 - accuracy: 0.8701\n",
      "Epoch 2: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.3731 - accuracy: 0.8701 - val_loss: 0.5169 - val_accuracy: 0.8308\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3693 - accuracy: 0.8723\n",
      "Epoch 3: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3693 - accuracy: 0.8723 - val_loss: 0.4955 - val_accuracy: 0.8372\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.8731\n",
      "Epoch 4: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3670 - accuracy: 0.8731 - val_loss: 0.5225 - val_accuracy: 0.8293\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8732\n",
      "Epoch 5: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3613 - accuracy: 0.8732 - val_loss: 0.6838 - val_accuracy: 0.7904\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.8746\n",
      "Epoch 6: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3586 - accuracy: 0.8746 - val_loss: 0.5626 - val_accuracy: 0.8253\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3552 - accuracy: 0.8769\n",
      "Epoch 7: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3552 - accuracy: 0.8769 - val_loss: 0.6460 - val_accuracy: 0.7997\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.8766\n",
      "Epoch 8: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3528 - accuracy: 0.8766 - val_loss: 0.4857 - val_accuracy: 0.8395\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.8780\n",
      "Epoch 9: val_loss did not improve from 0.45637\n",
      "390/390 [==============================] - 221s 564ms/step - loss: 0.3491 - accuracy: 0.8780 - val_loss: 0.4840 - val_accuracy: 0.8417\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8811\n",
      "Epoch 10: val_loss improved from 0.45637 to 0.45521, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-10.hdf5\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.3449 - accuracy: 0.8811 - val_loss: 0.4552 - val_accuracy: 0.8543\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8793\n",
      "Epoch 11: val_loss improved from 0.45521 to 0.44630, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-11.hdf5\n",
      "390/390 [==============================] - 227s 581ms/step - loss: 0.3461 - accuracy: 0.8793 - val_loss: 0.4463 - val_accuracy: 0.8558\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8815\n",
      "Epoch 12: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.3401 - accuracy: 0.8815 - val_loss: 0.4823 - val_accuracy: 0.8420\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.8825\n",
      "Epoch 13: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3341 - accuracy: 0.8825 - val_loss: 0.5320 - val_accuracy: 0.8309\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3337 - accuracy: 0.8839\n",
      "Epoch 14: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3337 - accuracy: 0.8839 - val_loss: 0.5665 - val_accuracy: 0.8238\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8831\n",
      "Epoch 15: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.3306 - accuracy: 0.8831 - val_loss: 0.5134 - val_accuracy: 0.8364\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8866\n",
      "Epoch 16: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3253 - accuracy: 0.8866 - val_loss: 0.6305 - val_accuracy: 0.8137\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8864\n",
      "Epoch 17: val_loss did not improve from 0.44630\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3258 - accuracy: 0.8864 - val_loss: 0.5215 - val_accuracy: 0.8439\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 0.8887\n",
      "Epoch 18: val_loss improved from 0.44630 to 0.39633, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-18.hdf5\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.3187 - accuracy: 0.8887 - val_loss: 0.3963 - val_accuracy: 0.8682\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.8880\n",
      "Epoch 19: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.3220 - accuracy: 0.8880 - val_loss: 0.4430 - val_accuracy: 0.8566\n",
      "Epoch 20/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.8886\n",
      "Epoch 20: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3186 - accuracy: 0.8886 - val_loss: 0.4427 - val_accuracy: 0.8572\n",
      "Epoch 21/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8915\n",
      "Epoch 21: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3113 - accuracy: 0.8915 - val_loss: 0.5639 - val_accuracy: 0.8286\n",
      "Epoch 22/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8921\n",
      "Epoch 22: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3128 - accuracy: 0.8921 - val_loss: 0.4105 - val_accuracy: 0.8689\n",
      "Epoch 23/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8927\n",
      "Epoch 23: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.3089 - accuracy: 0.8927 - val_loss: 0.4282 - val_accuracy: 0.8578\n",
      "Epoch 24/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.8924\n",
      "Epoch 24: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 564ms/step - loss: 0.3071 - accuracy: 0.8924 - val_loss: 0.4575 - val_accuracy: 0.8562\n",
      "Epoch 25/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8940\n",
      "Epoch 25: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3010 - accuracy: 0.8940 - val_loss: 0.4125 - val_accuracy: 0.8671\n",
      "Epoch 26/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8948\n",
      "Epoch 26: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.3011 - accuracy: 0.8948 - val_loss: 0.5303 - val_accuracy: 0.8403\n",
      "Epoch 27/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8954\n",
      "Epoch 27: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 564ms/step - loss: 0.2990 - accuracy: 0.8954 - val_loss: 0.4773 - val_accuracy: 0.8536\n",
      "Epoch 28/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8966\n",
      "Epoch 28: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.2959 - accuracy: 0.8966 - val_loss: 0.4488 - val_accuracy: 0.8526\n",
      "Epoch 29/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.8969\n",
      "Epoch 29: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2945 - accuracy: 0.8969 - val_loss: 0.5145 - val_accuracy: 0.8398\n",
      "Epoch 30/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2889 - accuracy: 0.8982\n",
      "Epoch 30: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2889 - accuracy: 0.8982 - val_loss: 0.4455 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29bff91250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restoring the last model\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-25.hdf5')\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,earlystop])\n",
    "# model.save_weights(os.path.join(path, '72epochs.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0PiVqoCny6V",
    "outputId": "40f05953-3130-430b-e820-c97c57a6b98f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - ETA: 0s - loss: 0.3185 - accuracy: 0.8885\n",
      "Epoch 1: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 225s 567ms/step - loss: 0.3185 - accuracy: 0.8885 - val_loss: 0.5511 - val_accuracy: 0.8284\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.8898\n",
      "Epoch 2: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3161 - accuracy: 0.8898 - val_loss: 0.4796 - val_accuracy: 0.8521\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3117 - accuracy: 0.8910\n",
      "Epoch 3: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.3117 - accuracy: 0.8910 - val_loss: 0.4102 - val_accuracy: 0.8654\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8929\n",
      "Epoch 4: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3118 - accuracy: 0.8929 - val_loss: 0.4436 - val_accuracy: 0.8581\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8919\n",
      "Epoch 5: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3099 - accuracy: 0.8919 - val_loss: 0.4557 - val_accuracy: 0.8548\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.8931\n",
      "Epoch 6: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3055 - accuracy: 0.8931 - val_loss: 0.5963 - val_accuracy: 0.8252\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3037 - accuracy: 0.8932\n",
      "Epoch 7: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3037 - accuracy: 0.8932 - val_loss: 0.4700 - val_accuracy: 0.8559\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8942\n",
      "Epoch 8: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.3029 - accuracy: 0.8942 - val_loss: 0.5278 - val_accuracy: 0.8433\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8952\n",
      "Epoch 9: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.2958 - accuracy: 0.8952 - val_loss: 0.4386 - val_accuracy: 0.8640\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8956\n",
      "Epoch 10: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2963 - accuracy: 0.8956 - val_loss: 0.4502 - val_accuracy: 0.8553\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8975\n",
      "Epoch 11: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2949 - accuracy: 0.8975 - val_loss: 0.4397 - val_accuracy: 0.8597\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8967\n",
      "Epoch 12: val_loss did not improve from 0.39633\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2943 - accuracy: 0.8967 - val_loss: 0.4244 - val_accuracy: 0.8659\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8999\n",
      "Epoch 13: val_loss improved from 0.39633 to 0.37522, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-13.hdf5\n",
      "390/390 [==============================] - 227s 582ms/step - loss: 0.2878 - accuracy: 0.8999 - val_loss: 0.3752 - val_accuracy: 0.8800\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8978\n",
      "Epoch 14: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2890 - accuracy: 0.8978 - val_loss: 0.4160 - val_accuracy: 0.8636\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9001\n",
      "Epoch 15: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2854 - accuracy: 0.9001 - val_loss: 0.4054 - val_accuracy: 0.8700\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9009\n",
      "Epoch 16: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2822 - accuracy: 0.9009 - val_loss: 0.5322 - val_accuracy: 0.8464\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2840 - accuracy: 0.9011\n",
      "Epoch 17: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2840 - accuracy: 0.9011 - val_loss: 0.3917 - val_accuracy: 0.8752\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9035\n",
      "Epoch 18: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 221s 565ms/step - loss: 0.2758 - accuracy: 0.9035 - val_loss: 0.4572 - val_accuracy: 0.8592\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9024\n",
      "Epoch 19: val_loss did not improve from 0.37522\n",
      "390/390 [==============================] - 226s 579ms/step - loss: 0.2808 - accuracy: 0.9024 - val_loss: 0.3949 - val_accuracy: 0.8706\n",
      "Epoch 20/30\n",
      " 85/390 [=====>........................] - ETA: 2:40 - loss: 0.2684 - accuracy: 0.9052"
     ]
    }
   ],
   "source": [
    "#restoring the last model\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-18.hdf5')\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,earlystop])\n",
    "# model.save_weights(os.path.join(path, '72epochs.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6536245,
     "status": "ok",
     "timestamp": 1650993178909,
     "user": {
      "displayName": "Akhil Matcha",
      "userId": "04170140370466070071"
     },
     "user_tz": -330
    },
    "id": "hWv4r60CCXPb",
    "outputId": "cc7dffc7-9827-4a5c-ed13-e43ea1af4838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9122\n",
      "Epoch 1: val_loss improved from 0.38042 to 0.33429, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-01.hdf5\n",
      "390/390 [==============================] - 250s 555ms/step - loss: 0.2495 - accuracy: 0.9122 - val_loss: 0.3343 - val_accuracy: 0.8899\n",
      "Epoch 2/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9171\n",
      "Epoch 2: val_loss improved from 0.33429 to 0.32902, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-02.hdf5\n",
      "390/390 [==============================] - 216s 552ms/step - loss: 0.2372 - accuracy: 0.9171 - val_loss: 0.3290 - val_accuracy: 0.8919\n",
      "Epoch 3/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2333 - accuracy: 0.9195\n",
      "Epoch 3: val_loss did not improve from 0.32902\n",
      "390/390 [==============================] - 220s 564ms/step - loss: 0.2333 - accuracy: 0.9195 - val_loss: 0.3357 - val_accuracy: 0.8903\n",
      "Epoch 4/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9175\n",
      "Epoch 4: val_loss did not improve from 0.32902\n",
      "390/390 [==============================] - 214s 548ms/step - loss: 0.2366 - accuracy: 0.9175 - val_loss: 0.3306 - val_accuracy: 0.8915\n",
      "Epoch 5/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9219\n",
      "Epoch 5: val_loss did not improve from 0.32902\n",
      "390/390 [==============================] - 219s 562ms/step - loss: 0.2274 - accuracy: 0.9219 - val_loss: 0.3314 - val_accuracy: 0.8927\n",
      "Epoch 6/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9200\n",
      "Epoch 6: val_loss improved from 0.32902 to 0.32415, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-06.hdf5\n",
      "390/390 [==============================] - 214s 549ms/step - loss: 0.2287 - accuracy: 0.9200 - val_loss: 0.3241 - val_accuracy: 0.8950\n",
      "Epoch 7/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.9201\n",
      "Epoch 7: val_loss improved from 0.32415 to 0.32166, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-07.hdf5\n",
      "390/390 [==============================] - 216s 552ms/step - loss: 0.2296 - accuracy: 0.9201 - val_loss: 0.3217 - val_accuracy: 0.8951\n",
      "Epoch 8/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9216\n",
      "Epoch 8: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2266 - accuracy: 0.9216 - val_loss: 0.3255 - val_accuracy: 0.8939\n",
      "Epoch 9/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9228\n",
      "Epoch 9: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 562ms/step - loss: 0.2232 - accuracy: 0.9228 - val_loss: 0.3293 - val_accuracy: 0.8930\n",
      "Epoch 10/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9233\n",
      "Epoch 10: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 214s 547ms/step - loss: 0.2206 - accuracy: 0.9233 - val_loss: 0.3356 - val_accuracy: 0.8910\n",
      "Epoch 11/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9207\n",
      "Epoch 11: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 562ms/step - loss: 0.2244 - accuracy: 0.9207 - val_loss: 0.3339 - val_accuracy: 0.8929\n",
      "Epoch 12/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9242\n",
      "Epoch 12: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 562ms/step - loss: 0.2189 - accuracy: 0.9242 - val_loss: 0.3281 - val_accuracy: 0.8935\n",
      "Epoch 13/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9229\n",
      "Epoch 13: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 214s 547ms/step - loss: 0.2196 - accuracy: 0.9229 - val_loss: 0.3278 - val_accuracy: 0.8934\n",
      "Epoch 14/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9234\n",
      "Epoch 14: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 214s 547ms/step - loss: 0.2193 - accuracy: 0.9234 - val_loss: 0.3306 - val_accuracy: 0.8920\n",
      "Epoch 15/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9217\n",
      "Epoch 15: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2198 - accuracy: 0.9217 - val_loss: 0.3471 - val_accuracy: 0.8870\n",
      "Epoch 16/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9248\n",
      "Epoch 16: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2134 - accuracy: 0.9248 - val_loss: 0.3447 - val_accuracy: 0.8888\n",
      "Epoch 17/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9252\n",
      "Epoch 17: val_loss did not improve from 0.32166\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2151 - accuracy: 0.9252 - val_loss: 0.3240 - val_accuracy: 0.8951\n",
      "Epoch 18/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9225\n",
      "Epoch 18: val_loss improved from 0.32166 to 0.31959, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-18.hdf5\n",
      "390/390 [==============================] - 215s 551ms/step - loss: 0.2194 - accuracy: 0.9225 - val_loss: 0.3196 - val_accuracy: 0.8951\n",
      "Epoch 19/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9255\n",
      "Epoch 19: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 213s 546ms/step - loss: 0.2141 - accuracy: 0.9255 - val_loss: 0.3365 - val_accuracy: 0.8915\n",
      "Epoch 20/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9246\n",
      "Epoch 20: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 213s 546ms/step - loss: 0.2171 - accuracy: 0.9246 - val_loss: 0.3260 - val_accuracy: 0.8941\n",
      "Epoch 21/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9258\n",
      "Epoch 21: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 213s 546ms/step - loss: 0.2136 - accuracy: 0.9258 - val_loss: 0.3258 - val_accuracy: 0.8942\n",
      "Epoch 22/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9260\n",
      "Epoch 22: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2125 - accuracy: 0.9260 - val_loss: 0.3308 - val_accuracy: 0.8947\n",
      "Epoch 23/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2121 - accuracy: 0.9254\n",
      "Epoch 23: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2121 - accuracy: 0.9254 - val_loss: 0.3215 - val_accuracy: 0.8952\n",
      "Epoch 24/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9256\n",
      "Epoch 24: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2138 - accuracy: 0.9256 - val_loss: 0.3451 - val_accuracy: 0.8910\n",
      "Epoch 25/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2139 - accuracy: 0.9246\n",
      "Epoch 25: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 214s 546ms/step - loss: 0.2139 - accuracy: 0.9246 - val_loss: 0.3508 - val_accuracy: 0.8895\n",
      "Epoch 26/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9267\n",
      "Epoch 26: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 213s 546ms/step - loss: 0.2120 - accuracy: 0.9267 - val_loss: 0.3282 - val_accuracy: 0.8942\n",
      "Epoch 27/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9258\n",
      "Epoch 27: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 214s 547ms/step - loss: 0.2120 - accuracy: 0.9258 - val_loss: 0.3302 - val_accuracy: 0.8932\n",
      "Epoch 28/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9256\n",
      "Epoch 28: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 219s 561ms/step - loss: 0.2106 - accuracy: 0.9256 - val_loss: 0.3371 - val_accuracy: 0.8903\n",
      "Epoch 29/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9266\n",
      "Epoch 29: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 220s 563ms/step - loss: 0.2094 - accuracy: 0.9266 - val_loss: 0.3287 - val_accuracy: 0.8947\n",
      "Epoch 30/30\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9258\n",
      "Epoch 30: val_loss did not improve from 0.31959\n",
      "390/390 [==============================] - 214s 547ms/step - loss: 0.2114 - accuracy: 0.9258 - val_loss: 0.3356 - val_accuracy: 0.8933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f495b7c5550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restoring the last model\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-13.hdf5')\n",
    "keras.backend.set_value(model.optimizer.momentum, 0.7)\n",
    "keras.backend.set_value(model.optimizer.lr, 0.001)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,earlystop])\n",
    "# model.save_weights(os.path.join(path, '72epochs.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4537099,
     "status": "ok",
     "timestamp": 1650999963931,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "iQN6AgxoUYRF",
    "outputId": "417ef2e8-a00f-474b-b6f2-7d910af5fa85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9395\n",
      "Epoch 1: val_loss improved from inf to 0.31265, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-01.hdf5\n",
      "390/390 [==============================] - 247s 589ms/step - loss: 0.1749 - accuracy: 0.9395 - val_loss: 0.3126 - val_accuracy: 0.8999\n",
      "Epoch 2/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9414\n",
      "Epoch 2: val_loss improved from 0.31265 to 0.30632, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-02.hdf5\n",
      "390/390 [==============================] - 229s 585ms/step - loss: 0.1706 - accuracy: 0.9414 - val_loss: 0.3063 - val_accuracy: 0.9032\n",
      "Epoch 3/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9415\n",
      "Epoch 3: val_loss did not improve from 0.30632\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1691 - accuracy: 0.9415 - val_loss: 0.3094 - val_accuracy: 0.8975\n",
      "Epoch 4/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9405\n",
      "Epoch 4: val_loss did not improve from 0.30632\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.1692 - accuracy: 0.9405 - val_loss: 0.3139 - val_accuracy: 0.8996\n",
      "Epoch 5/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9430\n",
      "Epoch 5: val_loss did not improve from 0.30632\n",
      "390/390 [==============================] - 227s 581ms/step - loss: 0.1654 - accuracy: 0.9430 - val_loss: 0.3158 - val_accuracy: 0.8978\n",
      "Epoch 6/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9423\n",
      "Epoch 6: val_loss improved from 0.30632 to 0.30626, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-06.hdf5\n",
      "390/390 [==============================] - 228s 583ms/step - loss: 0.1642 - accuracy: 0.9423 - val_loss: 0.3063 - val_accuracy: 0.9005\n",
      "Epoch 7/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9434\n",
      "Epoch 7: val_loss did not improve from 0.30626\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1642 - accuracy: 0.9434 - val_loss: 0.3146 - val_accuracy: 0.8988\n",
      "Epoch 8/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9430\n",
      "Epoch 8: val_loss did not improve from 0.30626\n",
      "390/390 [==============================] - 227s 581ms/step - loss: 0.1642 - accuracy: 0.9430 - val_loss: 0.3066 - val_accuracy: 0.9020\n",
      "Epoch 9/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9447\n",
      "Epoch 9: val_loss improved from 0.30626 to 0.29956, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-09.hdf5\n",
      "390/390 [==============================] - 223s 570ms/step - loss: 0.1608 - accuracy: 0.9447 - val_loss: 0.2996 - val_accuracy: 0.9032\n",
      "Epoch 10/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9434\n",
      "Epoch 10: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.1652 - accuracy: 0.9434 - val_loss: 0.3017 - val_accuracy: 0.9034\n",
      "Epoch 11/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9451\n",
      "Epoch 11: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.1558 - accuracy: 0.9451 - val_loss: 0.3113 - val_accuracy: 0.9006\n",
      "Epoch 12/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1620 - accuracy: 0.9441\n",
      "Epoch 12: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1620 - accuracy: 0.9441 - val_loss: 0.3021 - val_accuracy: 0.9028\n",
      "Epoch 13/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9447\n",
      "Epoch 13: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 227s 582ms/step - loss: 0.1593 - accuracy: 0.9447 - val_loss: 0.3092 - val_accuracy: 0.9017\n",
      "Epoch 14/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9454\n",
      "Epoch 14: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1586 - accuracy: 0.9454 - val_loss: 0.3231 - val_accuracy: 0.8994\n",
      "Epoch 15/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9446\n",
      "Epoch 15: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1591 - accuracy: 0.9446 - val_loss: 0.3158 - val_accuracy: 0.9007\n",
      "Epoch 16/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9448\n",
      "Epoch 16: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 227s 581ms/step - loss: 0.1592 - accuracy: 0.9448 - val_loss: 0.3143 - val_accuracy: 0.8994\n",
      "Epoch 17/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9461\n",
      "Epoch 17: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1553 - accuracy: 0.9461 - val_loss: 0.3035 - val_accuracy: 0.9034\n",
      "Epoch 18/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9450\n",
      "Epoch 18: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 222s 567ms/step - loss: 0.1547 - accuracy: 0.9450 - val_loss: 0.3171 - val_accuracy: 0.9006\n",
      "Epoch 19/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9454\n",
      "Epoch 19: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.1566 - accuracy: 0.9454 - val_loss: 0.3100 - val_accuracy: 0.9025\n",
      "Epoch 20/20\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1547 - accuracy: 0.9457\n",
      "Epoch 20: val_loss did not improve from 0.29956\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.1547 - accuracy: 0.9457 - val_loss: 0.3176 - val_accuracy: 0.9004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0aaee61d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restoring the last model\n",
    "from keras.models import load_model\n",
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-07.hdf5')\n",
    "datagen = ImageDataGenerator(rotation_range=15,width_shift_range=0.1,height_shift_range=0.1,featurewise_std_normalization=True,horizontal_flip=True,fill_mode='nearest',zoom_range=0.2,shear_range=0.2)\n",
    "gen_flow = datagen.flow(X_train,y_train,batch_size=batch_size)\n",
    "# keras.backend.set_value(model.optimizer.momentum, 0.99)\n",
    "# keras.backend.set_value(model.optimizer.lr, 0.001)\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 20, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2285660,
     "status": "ok",
     "timestamp": 1651002261422,
     "user": {
      "displayName": "MachineLearning DeepLearning",
      "userId": "10954740806574998262"
     },
     "user_tz": -330
    },
    "id": "qI57dH1Q2S0-",
    "outputId": "642f9b68-cc8c-4ff5-f990-899ccb5bf2d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/390 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9577\n",
      "Epoch 1: val_loss improved from inf to 0.29507, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-01.hdf5\n",
      "390/390 [==============================] - 228s 575ms/step - loss: 0.1211 - accuracy: 0.9577 - val_loss: 0.2951 - val_accuracy: 0.9040 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n",
      "Epoch 2/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9586\n",
      "Epoch 2: val_loss improved from 0.29507 to 0.29452, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-02.hdf5\n",
      "390/390 [==============================] - 223s 570ms/step - loss: 0.1223 - accuracy: 0.9586 - val_loss: 0.2945 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "every 5th epoch learning rate is decreased by 5%\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 8.999999772640876e-05.\n",
      "Epoch 3/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9583\n",
      "Epoch 3: val_loss did not improve from 0.29452\n",
      "390/390 [==============================] - 227s 582ms/step - loss: 0.1221 - accuracy: 0.9583 - val_loss: 0.2949 - val_accuracy: 0.9050 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 9.000000136438757e-05.\n",
      "Epoch 4/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9584\n",
      "Epoch 4: val_loss improved from 0.29452 to 0.29397, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-04.hdf5\n",
      "390/390 [==============================] - 223s 570ms/step - loss: 0.1205 - accuracy: 0.9584 - val_loss: 0.2940 - val_accuracy: 0.9054 - lr: 9.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 9.000000136438757e-05.\n",
      "Epoch 5/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9594\n",
      "Epoch 5: val_loss improved from 0.29397 to 0.29391, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-05.hdf5\n",
      "390/390 [==============================] - 228s 583ms/step - loss: 0.1205 - accuracy: 0.9594 - val_loss: 0.2939 - val_accuracy: 0.9055 - lr: 9.0000e-05\n",
      "every 5th epoch learning rate is decreased by 5%\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 8.100000122794882e-05.\n",
      "Epoch 6/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9598\n",
      "Epoch 6: val_loss did not improve from 0.29391\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.1176 - accuracy: 0.9598 - val_loss: 0.2953 - val_accuracy: 0.9050 - lr: 8.1000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 8.099999831756577e-05.\n",
      "Epoch 7/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9599\n",
      "Epoch 7: val_loss did not improve from 0.29391\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.1170 - accuracy: 0.9599 - val_loss: 0.2952 - val_accuracy: 0.9054 - lr: 8.1000e-05\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 8.099999831756577e-05.\n",
      "Epoch 8/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9601\n",
      "Epoch 8: val_loss did not improve from 0.29391\n",
      "390/390 [==============================] - 221s 566ms/step - loss: 0.1188 - accuracy: 0.9601 - val_loss: 0.2941 - val_accuracy: 0.9050 - lr: 8.1000e-05\n",
      "every 5th epoch learning rate is decreased by 5%\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 7.289999848580919e-05.\n",
      "Epoch 9/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9590\n",
      "Epoch 9: val_loss improved from 0.29391 to 0.29334, saving model to /content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-09.hdf5\n",
      "390/390 [==============================] - 228s 584ms/step - loss: 0.1194 - accuracy: 0.9590 - val_loss: 0.2933 - val_accuracy: 0.9055 - lr: 7.2900e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 7.2900002123788e-05.\n",
      "Epoch 10/10\n",
      "391/390 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9587\n",
      "Epoch 10: val_loss did not improve from 0.29334\n",
      "390/390 [==============================] - 221s 567ms/step - loss: 0.1206 - accuracy: 0.9587 - val_loss: 0.2942 - val_accuracy: 0.9051 - lr: 7.2900e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0a8dd7c50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model('/content/drive/MyDrive/24-Assignment/Saved_Models/Model-1/Model1-weights-09.hdf5')\n",
    "keras.backend.set_value(model.optimizer.lr, 0.0001)\n",
    "keras.backend.set_value(model.optimizer.momentum, 0.5)\n",
    "bacth_size = 256\n",
    "datagen = ImageDataGenerator(rotation_range = 5, horizontal_flip = True, width_shift_range = 0.05, height_shift_range = 0.05, shear_range = 5)\n",
    "gen_flow = datagen.flow(X_train,y_train,batch_size=batch_size)\n",
    "model.fit_generator(gen_flow, steps_per_epoch = X_train.shape[0]/batch_size, epochs = 10, validation_data =(X_test, y_test), callbacks = [checkpoint,term_nan,lr_scheduled,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx5ia2AdY260"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN on CIFR Assignment .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
